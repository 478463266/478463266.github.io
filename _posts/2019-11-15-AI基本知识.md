---
layout: post
title: AI基本知识
categories: [学术]
description: AI基本概念
keywords: AI
---

1. 梯度下降法；

​    梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：**批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）**。

​	**批量梯度下降法**是最原始的形式，它是指在**每一次迭代时**使用**所有样本**来进行梯度的更新。
​	优点：（1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。（2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。
​	缺点：（1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。从迭代的次数上来看，BGD迭代的次数相对较少。

​	**随机梯度下降法**不同于批量梯度下降，随机梯度下降是**每次迭代**使用**一个样本**来对参数进行更新。使得训练速度加快。

​	**小批量梯度下降**，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：**每次迭代** 使用 ** batch_size** 个样本来对参数进行更新。

​	具体介绍：https://www.cnblogs.com/lliuye/p/9451903.html

2.坐标下降与块坐标下降

​	**坐标下降法**（coordinate descent）是一种非梯度优化算法。算法在每次迭代中，在当前点处沿一个坐标方向进行一维搜索以求得一个函数的局部极小值。在整个过程中循环使用不同的坐标方向。对于不可拆分的函数而言，算法可能无法在较小的迭代步数中求得最优解。为了加速收敛，可以采用一个适当的坐标系，例如通过主成分分析获得一个坐标间尽可能不相互关联的新坐标系。

​	将所有变量分组，然后在目标函数中惩罚每一组的L2范数，这样达到的效果就是可以将一整组的系数同时消成零，即抹掉一整组的变量，这种手法叫做Group Lasso 分组最小角回归算法。