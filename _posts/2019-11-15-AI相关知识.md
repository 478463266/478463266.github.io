---
layout: post
title: AI基本知识
categories: [学术]
description: AI基本概念
keywords: AI
---

1. 梯度下降法；

​    梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：**批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）**。

​	**批量梯度下降法**是最原始的形式，它是指在**每一次迭代时**使用**所有样本**来进行梯度的更新。
​	优点：（1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。（2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。
​	缺点：（1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。从迭代的次数上来看，BGD迭代的次数相对较少。

​	**随机梯度下降法**不同于批量梯度下降，随机梯度下降是**每次迭代**使用**一个样本**来对参数进行更新。使得训练速度加快。

​	**小批量梯度下降**，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：**每次迭代** 使用 ** batch_size** 个样本来对参数进行更新。

​	具体介绍：https://www.cnblogs.com/lliuye/p/9451903.html

2.坐标下降与块坐标下降

​	**坐标下降法**（coordinate descent）是一种非梯度优化算法。算法在每次迭代中，在当前点处沿一个坐标方向进行一维搜索以求得一个函数的局部极小值。在整个过程中循环使用不同的坐标方向。对于不可拆分的函数而言，算法可能无法在较小的迭代步数中求得最优解。为了加速收敛，可以采用一个适当的坐标系，例如通过主成分分析获得一个坐标间尽可能不相互关联的新坐标系。

​	将所有变量分组，然后在目标函数中惩罚每一组的L2范数，这样达到的效果就是可以将一整组的系数同时消成零，即抹掉一整组的变量，这种手法叫做Group Lasso 分组最小角回归算法。

3.联邦学习Federated learning

​	联邦学习（Federated Learning）是一种新兴的人工智能基础技术，在 2016 年由谷歌最先提出，原本用于解决安卓手机终端用户在本地更新模型的问题，其设计目标是在保障大数据交换时的信息安全、保护终端数据和个人数据隐私、保证合法合规的前提下，在多参与方或多计算结点之间开展高效率的机器学习。其中，联邦学习可使用的机器学习算法不局限于神经网络，还包括随机森林等重要算法。联邦学习有望成为下一代人工智能**协同**算法和**协作网络**的基础。

​	主要步骤：加密样本对齐；加密模型训练；效果激励

​	优势：

​	（1）数据隔离，数据不会泄露到外部，满足用户隐私保护和数据安全的需求；

​	（2）能够保证模型质量无损，不会出现负迁移，保证联邦模型比割裂的独立模型效果好；

​	（3）参与者地位对等，能够实现公平合作；

​	（4）能够保证参与各方在保持独立性的情况下，进行信息与模型参数的加密交换，并同时获得成长

​	分类：针对不同数据集，联邦学习分为横向联邦学习（horizontal federated learning）、纵向联邦学习（vertical federated learning）与联邦迁移学习（Federated Transfer Learning，FmL